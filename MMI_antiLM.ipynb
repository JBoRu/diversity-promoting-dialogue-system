{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import preprocess\n",
    "from DataLoader import DataLoader\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total words: 51805\n",
      "Dictionary cover ratio: 0.9796814762021151\n"
     ]
    }
   ],
   "source": [
    "# twitter dataset\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "max_vocab_size = 20000\n",
    "max_sen, min_sen = 14, 3\n",
    "unk_most = 2\n",
    "reverse_flag = 1  # reverse the input sequence order Sutskever et al., 2014\n",
    "inverse_flag = 0  # MMI bidirection: train P(T|S) by inversing source and target\n",
    "\n",
    "dataStat = preprocess.dataPreProcess('dataset/twitter.txt', max_vocab_size, max_sen, min_sen, unk_most, reverse_flag, inverse_flag) \n",
    "print(\"Number of total words:\", dataStat.numOfWords)\n",
    "\n",
    "wordCount = sorted(dataStat.word2cnt.values(), reverse=True)\n",
    "print(\"Dictionary cover ratio:\", sum(wordCount[:max_vocab_size-4]) / sum(wordCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad or cut input and output sentence\n",
    "\n",
    "def padding(source, maxLen):\n",
    "    return np.pad(source[:maxLen],(0,max(0,maxLen-len(source))),'constant')\n",
    "\n",
    "input_max_len, output_max_len = 15, 15\n",
    "\n",
    "pairsNum = len(dataStat.pairsInd)\n",
    "pairsLength = np.array([[len(l[0]), len(l[1])] for l in dataStat.pairsInd])\n",
    "upperLength = np.concatenate((np.ones((pairsNum,1), dtype=int)*input_max_len, \n",
    "                              np.ones((pairsNum,1), dtype=int)*output_max_len), axis=1)\n",
    "pairsLength = np.minimum(pairsLength,upperLength)\n",
    "pairsAligned = np.array([np.concatenate((padding(l[0], input_max_len), \n",
    "                                              padding(l[1], output_max_len))) for l in dataStat.pairsInd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs:  104010\n",
      "Total develop pairs:  5778\n",
      "Total test pairs:  5779\n"
     ]
    }
   ],
   "source": [
    "train_type = 'resume'\n",
    "\n",
    "ratios = [0.90, 0.05, 0.05]\n",
    "pairsNumTrain, pairsNumDeve = int(ratios[0]*pairsNum), int(ratios[1]*pairsNum)\n",
    "pairsNumTest = pairsNum - pairsNumTrain - pairsNumDeve\n",
    "\n",
    "if train_type=='restart':\n",
    "    deve_idxes = np.random.choice(pairsNum, pairsNumDeve, replace=False)\n",
    "    test_idxes = np.random.choice(list(set(np.arange(pairsNum)).difference(set(deve_idxes))), pairsNumTest, replace=False)\n",
    "    train_idxes = np.array(list(set(np.arange(pairsNum)).difference(set(deve_idxes)).difference(set(test_idxes))))\n",
    "    np.save(\"parameter/pairsIdxesTriple.npy\", (train_idxes, deve_idxes, test_idxes))\n",
    "else:\n",
    "    train_idxes, deve_idxes, test_idxes = np.load( \"parameter/pairsIdxesTriple.npy\" )\n",
    "\n",
    "pairsAlignedTrain, pairsAlignedDeve, pairsAlignedTest = pairsAligned[train_idxes], pairsAligned[deve_idxes], pairsAligned[test_idxes]\n",
    "pairsLengthTrain, pairsLengthDeve, pairsLengthTest = pairsLength[train_idxes], pairsLength[deve_idxes], pairsLength[test_idxes]\n",
    "    \n",
    "print(\"Total training pairs: \",pairsNumTrain)\n",
    "print(\"Total develop pairs: \",pairsNumDeve)\n",
    "print(\"Total test pairs: \",pairsNumTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny verification program\n",
    "\n",
    "# environment setup\n",
    "vocab_size = min(dataStat.numOfWords, max_vocab_size)\n",
    "hidden_size = 1000\n",
    "train_type = 'resume'\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "rnnEncoder = model.Encoder(embedding, vocab_size,20,hidden_size, n_layers=2, bidirectional=False, variable_lengths=True)\n",
    "rnnDecoder = model.Decoder(embedding, vocab_size,1,hidden_size, n_layers=2)\n",
    "\n",
    "if train_type.lower()=='restart': pass\n",
    "elif train_type.lower()=='resume':\n",
    "    para_name = 'twitter_0518_35'\n",
    "    embedding.load_state_dict(torch.load('parameter/embeding_'+para_name+'.pt'))\n",
    "    rnnEncoder.load_state_dict(torch.load('parameter/encoder_'+para_name+'.pt'))\n",
    "    rnnDecoder.load_state_dict(torch.load('parameter/decoder_'+para_name+'.pt'))\n",
    "else: print(\"Please enter valid training type !\")\n",
    "\n",
    "if USE_CUDA:\n",
    "    rnnEncoder.cuda()\n",
    "    rnnDecoder.cuda()\n",
    "\n",
    "criterion = nn.NLLLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration per epoch: 3250\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0002\n",
    "optimizer_encoder = optim.Adam(rnnEncoder.parameters(), learning_rate)\n",
    "optimizer_decoder = optim.Adam(rnnDecoder.parameters(), learning_rate)\n",
    "\n",
    "# initialize dataloader\n",
    "batch_size = 32\n",
    "trainLoader = DataLoader(pairsAlignedTrain, pairsLengthTrain, input_max_len, output_max_len)\n",
    "trainLoader.reset(batch_size)\n",
    "\n",
    "deveLoader = DataLoader(pairsAlignedDeve, pairsLengthDeve, input_max_len, output_max_len)\n",
    "deveLoader.reset(1)\n",
    "\n",
    "testLoader = DataLoader(pairsAlignedTest, pairsLengthTest, input_max_len, output_max_len)\n",
    "testLoader.reset(1)\n",
    "\n",
    "print(\"iteration per epoch:\", int(pairsNumTrain/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneMask(outputs_record, lengths):\n",
    "    batch_size = lengths.size(0)\n",
    "    # prepare\n",
    "    comp = torch.arange(output_max_len).view(-1,1)\n",
    "    if USE_CUDA: comp = comp.cuda()\n",
    "    comp_ex = comp.repeat(1,vocab_size).repeat(batch_size,1,1)\n",
    "    # generate\n",
    "    l_ex = lengths[:,1].view(batch_size,-1).repeat(1,vocab_size).view(batch_size,1,-1)\n",
    "    if USE_CUDA: l_ex = l_ex.type(torch.cuda.FloatTensor)\n",
    "    else: l_ex = l_ex.type(torch.FloatTensor)\n",
    "    mask = comp_ex < l_ex\n",
    "    if USE_CUDA: mask = mask.type(torch.cuda.FloatTensor)\n",
    "    else: mask = mask.type(torch.FloatTensor)\n",
    "    return torch.mul(mask, outputs_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneEpoch():\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_ind in range(int(pairsNum/batch_size)+1):\n",
    "    \n",
    "        # prepare mini-batch data\n",
    "        try:\n",
    "            inputs, targets, lengths = trainLoader.getMiniBatch()\n",
    "        except Exception as e:\n",
    "            # print('GG...')\n",
    "            break\n",
    "        else:\n",
    "            # print('Good!')\n",
    "            # Zero gradients of both optimizers\n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "\n",
    "            # encoding and decoding\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            hid_init = rnnEncoder.init_hidden(batch_size)\n",
    "            out_enc, hid_enc = rnnEncoder.forward(inputs,lengths[:,0],hid_init)\n",
    "            \n",
    "            # teacher forcing\n",
    "            hid_dec = hid_enc#[:rnnDecoder.n_layers]\n",
    "\n",
    "            # SOS_token\n",
    "            in_dec = Variable(torch.LongTensor([dataStat.word2ind['SOS']] * batch_size))\n",
    "            if USE_CUDA: in_dec = in_dec.cuda()\n",
    "            outputs_record, hid_dec = rnnDecoder.forward(in_dec.view(batch_size,-1),hid_dec)\n",
    "            # continue\n",
    "            for i in range(output_max_len-1):\n",
    "                out_dec, hid_dec = rnnDecoder.forward(targets[:,i].view(batch_size,-1),hid_dec)\n",
    "                outputs_record = torch.cat((outputs_record, out_dec), 1)\n",
    "\n",
    "            outputs_mask = geneMask(outputs_record, lengths)\n",
    "            loss = criterion(torch.transpose(outputs_mask,1,2), targets)\n",
    "            # print(loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "            optimizer_decoder.step()\n",
    "            \n",
    "            running_loss += float(loss)\n",
    "            \n",
    "            if (batch_ind+1)%1000 == 0:\n",
    "                print(\"iteration\", batch_ind+1, \" ---- running loss:\", running_loss/batch_ind)\n",
    "            \n",
    "    # print('running_loss:',running_loss)\n",
    "    return running_loss/batch_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePara(epoch):\n",
    "    para_name = 'twitter_0518_'+str(epoch)\n",
    "    torch.save(embedding.state_dict(),'parameter/embeding_'+para_name+'.pt')\n",
    "    torch.save(rnnEncoder.state_dict(),'parameter/encoder_'+para_name+'.pt')\n",
    "    torch.save(rnnDecoder.state_dict(),'parameter/decoder_'+para_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n",
      "Fri May 18 13:48:28 2018\n",
      "iteration 1000  ---- running loss: 0.7764374853015781\n",
      "iteration 2000  ---- running loss: 0.7936298665313377\n",
      "iteration 3000  ---- running loss: 0.8099324350636894\n",
      "Epoch: 1 \tLoss: 0.8131477450590867\n",
      "Fri May 18 13:53:55 2018\n",
      "iteration 1000  ---- running loss: 0.7073472350447982\n",
      "iteration 2000  ---- running loss: 0.7228136096315542\n",
      "iteration 3000  ---- running loss: 0.7357599485632021\n",
      "Epoch: 2 \tLoss: 0.7386495542526245\n",
      "Fri May 18 13:59:20 2018\n",
      "iteration 1000  ---- running loss: 0.6406484229547961\n",
      "iteration 2000  ---- running loss: 0.6553890522030844\n",
      "iteration 3000  ---- running loss: 0.6696273117869963\n",
      "Epoch: 3 \tLoss: 0.6734025310736436\n",
      "Fri May 18 14:04:46 2018\n",
      "iteration 1000  ---- running loss: 0.5867972803545428\n",
      "iteration 2000  ---- running loss: 0.597785772011243\n",
      "iteration 3000  ---- running loss: 0.6119136117962688\n",
      "Epoch: 4 \tLoss: 0.615221876676266\n",
      "Fri May 18 14:10:12 2018\n",
      "iteration 1000  ---- running loss: 0.5322195835657664\n",
      "iteration 2000  ---- running loss: 0.5470120981521998\n",
      "iteration 3000  ---- running loss: 0.5607763142158843\n",
      "Epoch: 5 \tLoss: 0.5638911216900899\n",
      "Fri May 18 14:15:38 2018\n",
      "iteration 1000  ---- running loss: 0.48991997022409217\n",
      "iteration 2000  ---- running loss: 0.5011361236420794\n",
      "iteration 3000  ---- running loss: 0.5149129932027692\n",
      "Epoch: 6 \tLoss: 0.517942050255262\n",
      "Fri May 18 14:21:05 2018\n",
      "iteration 1000  ---- running loss: 0.44768989658928493\n",
      "iteration 2000  ---- running loss: 0.4602323704626991\n",
      "iteration 3000  ---- running loss: 0.474189354016726\n",
      "Epoch: 7 \tLoss: 0.4770525205868941\n",
      "Fri May 18 14:26:32 2018\n",
      "iteration 1000  ---- running loss: 0.41324694787298477\n",
      "iteration 2000  ---- running loss: 0.42459340264583717\n",
      "iteration 3000  ---- running loss: 0.43749470045344757\n",
      "Epoch: 8 \tLoss: 0.4401566848663183\n",
      "Fri May 18 14:31:58 2018\n",
      "iteration 1000  ---- running loss: 0.38189873534876545\n",
      "iteration 2000  ---- running loss: 0.3933874129384324\n",
      "iteration 3000  ---- running loss: 0.4044909343178092\n",
      "Epoch: 9 \tLoss: 0.407024121761322\n",
      "Fri May 18 14:37:25 2018\n",
      "iteration 1000  ---- running loss: 0.3491632898708244\n",
      "iteration 2000  ---- running loss: 0.3612826618553699\n",
      "iteration 3000  ---- running loss: 0.37394261578791693\n",
      "Epoch: 10 \tLoss: 0.37677874757693364\n",
      "Fri May 18 14:42:52 2018\n",
      "iteration 1000  ---- running loss: 0.3265627006272057\n",
      "iteration 2000  ---- running loss: 0.33498411603633493\n",
      "iteration 3000  ---- running loss: 0.34613183506451756\n",
      "Epoch: 11 \tLoss: 0.34866021879819725\n",
      "Fri May 18 14:48:20 2018\n",
      "iteration 1000  ---- running loss: 0.3011653171525942\n",
      "iteration 2000  ---- running loss: 0.3102537785442905\n",
      "iteration 3000  ---- running loss: 0.3210819805958304\n",
      "Epoch: 12 \tLoss: 0.3237495224017363\n",
      "Fri May 18 14:53:46 2018\n",
      "iteration 1000  ---- running loss: 0.27587192127475507\n",
      "iteration 2000  ---- running loss: 0.28768844781755626\n",
      "iteration 3000  ---- running loss: 0.2977851469761612\n",
      "Epoch: 13 \tLoss: 0.300586718586775\n",
      "Fri May 18 14:59:13 2018\n",
      "iteration 1000  ---- running loss: 0.26020703335722406\n",
      "iteration 2000  ---- running loss: 0.2673546939551562\n",
      "iteration 3000  ---- running loss: 0.27730924378994826\n",
      "Epoch: 14 \tLoss: 0.27955965886666223\n",
      "Fri May 18 15:04:39 2018\n",
      "iteration 1000  ---- running loss: 0.23777974218696923\n",
      "iteration 2000  ---- running loss: 0.24809334893385251\n",
      "iteration 3000  ---- running loss: 0.25828283468696744\n",
      "Epoch: 15 \tLoss: 0.26072940016251345\n",
      "Fri May 18 15:10:10 2018\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training...\")\n",
    "print(time.asctime( time.localtime(time.time()) ))\n",
    "\n",
    "for i in range(10):\n",
    "    trainLoader.reset(batch_size)\n",
    "    loss = oneEpoch()\n",
    "    if (i+1)%1==0:\n",
    "        print('Epoch:', i+1, '\\tLoss:',loss)\n",
    "        print(time.asctime( time.localtime(time.time()) ))\n",
    "    if (i+1)%5==0:\n",
    "        savePara(i+1+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Language Model: P(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct P(target) language model\n",
    "\n",
    "freqLM = {}\n",
    "\n",
    "for i in range(pairsNumTrain):\n",
    "    length = pairsLengthTrain[i][1]\n",
    "    rsps = pairsAlignedTrain[i][input_max_len:input_max_len+length]\n",
    "    dic = freqLM\n",
    "    for j in range(min(5,length)):\n",
    "        if rsps[j] not in dic: dic[rsps[j]] = [1,{}]\n",
    "        else: dic[rsps[j]][0] += 1\n",
    "        dic = dic[rsps[j]][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conProb(prefix):\n",
    "    dic = freqLM\n",
    "    dist_array = np.ones(vocab_size)\n",
    "    try:\n",
    "        for ind in prefix:\n",
    "            dic = dic[ind][1]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    else:\n",
    "        for key in dic:\n",
    "            dist_array[key] += dic[key][0]\n",
    "    total_freq = np.sum(dist_array)\n",
    "    dist_tensor = torch.FloatTensor(np.log(dist_array/total_freq))\n",
    "    if USE_CUDA: dist_tensor = dist_tensor.cuda()\n",
    "    return dist_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute by BLEU and distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate into natural language\n",
    "\n",
    "def showResult(ind_seq, reverse=False):\n",
    "    token_list = []\n",
    "    for i in ind_seq:\n",
    "        if i == dataStat.word2ind['EOS']: break\n",
    "        token_list.append(dataStat.ind2word[i])\n",
    "    return ' '.join(token_list[::-1]) if reverse else  ' '.join(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topOneDecode(decoder, decoder_hidden, stat, max_length=output_max_len):\n",
    "    \n",
    "    decoder_input = torch.LongTensor([SOS_token]).view(1,-1)\n",
    "    if USE_CUDA: \n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_hidden = decoder_hidden.cuda()\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "    for di in range(max_length):\n",
    "\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append(ni.item())\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ni.item())\n",
    "\n",
    "        decoder_input = torch.LongTensor([[ni]])\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamDecode(decoder, decoder_hidden, voc, beam_size, lamda, threshold, max_length=output_max_len):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for _ in range(max_length):\n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, sentence.decoder_hidden)\n",
    "            \n",
    "            # apply MMI anti-language model\n",
    "            if len(sentence.sentence_idxes)<threshold:\n",
    "                LM_output = conProb([int(idx) for idx in sentence.sentence_idxes])\n",
    "                decoder_output -= lamda*LM_output.view(1,1,-1)\n",
    "            \n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "            \n",
    "        next_top_sentences.sort(key=lambda s: s.getScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 10)  # N-best list\n",
    "    return terminal_sentences[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def getScore(self, mode='sum', gamma=0.2):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            print(\"sentence of length 0\")\n",
    "            return torch.cuda.FloatTensor(-999) if USE_CUDA else torch.FloatTensor(-999)\n",
    "        if mode=='avg':\n",
    "            res = sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "        else:\n",
    "            res = sum(self.sentence_scores) + gamma*len(self.sentence_scores)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        terminates, sentences = [], []\n",
    "        \n",
    "        topi, topv = topi[0], topv[0]  # get data out of batch\n",
    "        \n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([int(idx) for idx in self.sentence_idxes] + [EOS_token],\n",
    "                                   self.getScore())) # tuple(word_list, score_float)\n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] # pass by value\n",
    "            scores = self.sentence_scores[:] # pass by value\n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append(EOS_token)\n",
    "            else:\n",
    "                words.append(int(self.sentence_idxes[i]))\n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append(EOS_token)\n",
    "        return (words, self.getScore())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input, length, encoder, decoder, beam_size, lamda=0.0, threshold=0, verbose=False):\n",
    "    # encoding and decoding\n",
    "    hid_init = encoder.init_hidden(batch_size = 1)\n",
    "    out_enc, hid_enc = encoder.forward(input.view(1,-1),length.view(1),hid_init)\n",
    "    hid_dec = hid_enc#[:rnnDecoder.n_layers]\n",
    "\n",
    "    if beam_size==0:\n",
    "        path = topOneDecode(decoder, hid_dec, dataStat, max_length=15)  # return path in list\n",
    "        return path\n",
    "    else:\n",
    "        path_beam = beamDecode(decoder, hid_dec, dataStat, beam_size, lamda, threshold)  # return list of tuples: (path, score)\n",
    "        if verbose:\n",
    "            for p in path_beam: print(float(p[1]), '\\t', showResult(p[0]))\n",
    "        return path_beam[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSample(encoder, decoder, beam_size=5, lamda=0.0, threshold=0, verbose=True, myQuery=''):\n",
    "    \n",
    "    if myQuery == '':\n",
    "        print(\"Blank Input\")\n",
    "        return -1\n",
    "    else:\n",
    "        # feed in customized tokens\n",
    "        sample_query = myQuery.lower()\n",
    "        sample_query_ind, _ = preprocess.encodePair(dataStat, (sample_query,'.'),reverse=True)\n",
    "        sample_query_tensor = torch.LongTensor([padding(sample_query_ind, input_max_len)])\n",
    "        sample_query_length = torch.LongTensor([len(sample_query_ind)])\n",
    "        if USE_CUDA: input, length, target = sample_query_tensor.cuda(), sample_query_length.cuda(), None\n",
    "        \n",
    "    trace = generate(input, length, encoder, decoder, beam_size, lamda, threshold, verbose=True)\n",
    "    if verbose:\n",
    "        print(\"Message:\\t\", showResult(input.data[0].cpu().numpy(), reverse=True))\n",
    "        print(\"Response:\\t\", showResult(trace))\n",
    "        if target is not None:\n",
    "            print(\"Teaching:\\t\", showResult(target.data[0].cpu().numpy()))\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def evaluateCorpus(encoder, decoder, beam_size=1, lamda=0.0, threshold=0, loader=testLoader, display=False):\n",
    "    \n",
    "    loader.reset(1)\n",
    "    data_length = loader.dataLength\n",
    "    responses = []\n",
    "    total_score = 0\n",
    "    sample_length = min(2000, data_length)\n",
    "    \n",
    "    for i in range(sample_length):\n",
    "        inputs, targets, lengths = loader.getMiniBatch()\n",
    "        input, length, target = inputs, lengths[0][0], targets\n",
    "\n",
    "        trace = generate(input, length, encoder, decoder, beam_size, lamda, threshold)\n",
    "        responses.append(trace)\n",
    "                \n",
    "        length_ref = lengths[0][1]\n",
    "        references = [[target.data[0].tolist()[:int(length_ref)]]]\n",
    "        candidates = [trace]\n",
    "        score = corpus_bleu(references, candidates, smoothing_function=chencherry.method1)\n",
    "        total_score += score\n",
    "        \n",
    "        if display and (i+1)%int(sample_length/10)==0: print(\"complete\",int(100*(i+1)/sample_length),\"%\")\n",
    "        \n",
    "    return total_score/i, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct evaluation\n",
    "\n",
    "import nltk\n",
    "\n",
    "def distinctEval(all_paths):\n",
    "\n",
    "    response_ugm = set([])\n",
    "    response_bgm = set([])        \n",
    "    response_len = sum([len(p) for p in all_paths])\n",
    "\n",
    "    for path in all_paths:\n",
    "        for u in path:\n",
    "            response_ugm.add(u)\n",
    "        for b in list(nltk.bigrams(path)):\n",
    "            response_bgm.add(b)\n",
    "\n",
    "    print(\"total length of response:\", response_len)\n",
    "    print(\"distinct unigrams:\", len(response_ugm)/response_len)\n",
    "    print(\"distinct bigrams:\", len(response_bgm)/response_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.576613664627075 \t love it ! can t wait\n",
      "-3.0073845386505127 \t blocked that guy .\n",
      "-3.1148147583007812 \t blocked that guy . thanks\n",
      "-3.1317696571350098 \t blocked that guy\n",
      "-3.5388383865356445 \t can t wait for it\n",
      "-3.5720245838165283 \t love it ! can t wait for the next one .\n",
      "-3.677725076675415 \t true ! but i m there\n",
      "-4.128273963928223 \t blocked that guy . thanks for the heads up\n",
      "-4.14186954498291 \t blocked that guy . thanks dude\n",
      "-4.328236103057861 \t blocked that guy . thanks for the heads\n",
      "Message:\t seriously one of the best shows ! ! !\n",
      "Response:\t love it ! can t wait\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateSample(rnnEncoder, rnnDecoder, 5, 0.2, 2, True, 'seriously one of the best shows ! ! !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 19 15:37:01 2018\n",
      "0 \t 0\n",
      "0.6612963325219124 0.037271626386776724 0.03731144918329606\n",
      "total length of response: 13744\n",
      "distinct unigrams: 0.10702852153667054\n",
      "distinct bigrams: 0.41916472642607683\n",
      "total length of response: 13780\n",
      "distinct unigrams: 0.10377358490566038\n",
      "distinct bigrams: 0.4126269956458636\n"
     ]
    }
   ],
   "source": [
    "# baseline -- gamma = 0, beam = 2\n",
    "lamda_list = [0]\n",
    "thres_list = [0]\n",
    "score_list = [[],[],[]]\n",
    "\n",
    "for threshold in thres_list:\n",
    "    for lamda in lamda_list:\n",
    "        print(time.asctime( time.localtime(time.time()) ))\n",
    "        print(threshold, '\\t', lamda)\n",
    "        score_train,_ = evaluateCorpus(rnnEncoder, rnnDecoder,2, lamda, threshold, loader=trainLoader, display=0)\n",
    "        score_deve,paths_deve = evaluateCorpus(rnnEncoder, rnnDecoder, 2, lamda, threshold, loader=deveLoader, display=0)\n",
    "        score_test,paths_test = evaluateCorpus(rnnEncoder, rnnDecoder, 2, lamda, threshold, loader=testLoader, display=0)\n",
    "        score_list[0].append(score_train)\n",
    "        score_list[1].append(score_deve)\n",
    "        score_list[2].append(score_test)\n",
    "        print(score_train, score_deve, score_test)\n",
    "        distinctEval(paths_deve)\n",
    "        distinctEval(paths_test)\n",
    "\n",
    "# score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 19 12:59:29 2018\n",
      "1 \t -0.2\n",
      "0.7922088458092297 0.039162222479560166 0.03701055174145775\n",
      "total length of response: 14590\n",
      "distinct unigrams: 0.10095956134338588\n",
      "distinct bigrams: 0.4038382453735435\n",
      "total length of response: 14911\n",
      "distinct unigrams: 0.10187110187110188\n",
      "distinct bigrams: 0.40916102206424787\n",
      "Sat May 19 13:05:17 2018\n",
      "1 \t 0\n",
      "0.8278269493974896 0.03775382131805592 0.037414530302966875\n",
      "total length of response: 14829\n",
      "distinct unigrams: 0.10378312765527008\n",
      "distinct bigrams: 0.4145930271764785\n",
      "total length of response: 14859\n",
      "distinct unigrams: 0.10532337303990848\n",
      "distinct bigrams: 0.4200821051214752\n",
      "Sat May 19 13:11:05 2018\n",
      "1 \t 0.2\n",
      "0.8239186581560382 0.03774870156079711 0.037956076614411444\n",
      "total length of response: 14767\n",
      "distinct unigrams: 0.11173562673528814\n",
      "distinct bigrams: 0.431908986253132\n",
      "total length of response: 14662\n",
      "distinct unigrams: 0.11219478925112536\n",
      "distinct bigrams: 0.4288637293684354\n",
      "Sat May 19 13:16:51 2018\n",
      "1 \t 0.5\n",
      "0.8034878953760112 0.03598719670261189 0.035585678372406476\n",
      "total length of response: 14509\n",
      "distinct unigrams: 0.12805844648149425\n",
      "distinct bigrams: 0.4564063684609553\n",
      "total length of response: 14332\n",
      "distinct unigrams: 0.12810493999441808\n",
      "distinct bigrams: 0.4594613452414178\n",
      "Sat May 19 13:22:34 2018\n",
      "2 \t -0.2\n",
      "0.7827251573679701 0.03880308002640115 0.03748045168435462\n",
      "total length of response: 14920\n",
      "distinct unigrams: 0.10040214477211797\n",
      "distinct bigrams: 0.39966487935656836\n",
      "total length of response: 14948\n",
      "distinct unigrams: 0.09907679957184908\n",
      "distinct bigrams: 0.39436713941664436\n",
      "Sat May 19 13:28:37 2018\n",
      "2 \t 0\n",
      "0.8279739633141086 0.038424801228639234 0.03598347840257457\n",
      "total length of response: 14786\n",
      "distinct unigrams: 0.10232652509130258\n",
      "distinct bigrams: 0.4162045177870959\n",
      "total length of response: 14783\n",
      "distinct unigrams: 0.10424135831698572\n",
      "distinct bigrams: 0.4242034769667862\n",
      "Sat May 19 13:34:28 2018\n",
      "2 \t 0.2\n",
      "0.8197264580796196 0.03967488952280883 0.04033834899179139\n",
      "total length of response: 14491\n",
      "distinct unigrams: 0.11068939341660342\n",
      "distinct bigrams: 0.44807121661721067\n",
      "total length of response: 14609\n",
      "distinct unigrams: 0.11308097747963584\n",
      "distinct bigrams: 0.4454788144294613\n",
      "Sat May 19 13:40:21 2018\n",
      "2 \t 0.5\n"
     ]
    }
   ],
   "source": [
    "# hyper-prrameter tuning -- gamma is set as 0.25\n",
    "lamda_list = [-0.2,0,0.2,0.5]\n",
    "thres_list = [1,2]\n",
    "score_list = [[],[],[]]\n",
    "\n",
    "for threshold in thres_list:\n",
    "    for lamda in lamda_list:\n",
    "        print(time.asctime( time.localtime(time.time()) ))\n",
    "        print(threshold, '\\t', lamda)\n",
    "        score_train,_ = evaluateCorpus(rnnEncoder, rnnDecoder,2, lamda, threshold, loader=trainLoader, display=0)\n",
    "        score_deve,paths_deve = evaluateCorpus(rnnEncoder, rnnDecoder, 2, lamda, threshold, loader=deveLoader, display=0)\n",
    "        score_test,paths_test = evaluateCorpus(rnnEncoder, rnnDecoder, 2, lamda, threshold, loader=testLoader, display=0)\n",
    "        score_list[0].append(score_train)\n",
    "        score_list[1].append(score_deve)\n",
    "        score_list[2].append(score_test)\n",
    "        print(score_train, score_deve, score_test)\n",
    "        distinctEval(paths_deve)\n",
    "        distinctEval(paths_test)\n",
    "\n",
    "# score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 20 17:16:09 2018\n",
      "2 \t 0.1\n",
      "0.8709994793001742 0.04082476571928066 0.0450602922901014\n",
      "total length of response: 12569\n",
      "distinct unigrams: 0.1265017105577214\n",
      "distinct bigrams: 0.43607287771501313\n",
      "total length of response: 12468\n",
      "distinct unigrams: 0.13089509143407121\n",
      "distinct bigrams: 0.44048764837985244\n",
      "Sun May 20 17:36:31 2018\n",
      "2 \t 0.3\n",
      "0.8419612373889789 0.038244517589677354 0.040930512310572785\n",
      "total length of response: 12240\n",
      "distinct unigrams: 0.14910130718954248\n",
      "distinct bigrams: 0.4812091503267974\n",
      "total length of response: 12245\n",
      "distinct unigrams: 0.14789710085749286\n",
      "distinct bigrams: 0.4791343405471621\n",
      "Sun May 20 17:56:03 2018\n",
      "3 \t 0.1\n",
      "0.8611223711423327 0.03780930806804727 0.038418308167594076\n",
      "total length of response: 12635\n",
      "distinct unigrams: 0.12916501780767708\n",
      "distinct bigrams: 0.4460625247328848\n",
      "total length of response: 12549\n",
      "distinct unigrams: 0.12957207745637103\n",
      "distinct bigrams: 0.4450553828990358\n",
      "Sun May 20 18:16:02 2018\n",
      "3 \t 0.3\n",
      "0.8384625376860889 0.0383770596847083 0.03654694948092839\n",
      "total length of response: 12301\n",
      "distinct unigrams: 0.14941874644337858\n",
      "distinct bigrams: 0.49272416876676695\n",
      "total length of response: 12498\n",
      "distinct unigrams: 0.1485837734037446\n",
      "distinct bigrams: 0.495119219075052\n"
     ]
    }
   ],
   "source": [
    "# hyper-prrameter tuning\n",
    "lamda_list = [0.1, 0.3]  # gamma = 0.2\n",
    "thres_list = [2, 3]\n",
    "score_list = [[],[],[]]\n",
    "\n",
    "for threshold in thres_list:\n",
    "    for lamda in lamda_list:\n",
    "        print(time.asctime( time.localtime(time.time()) ))\n",
    "        print(threshold, '\\t', lamda)\n",
    "        score_train,_ = evaluateCorpus(rnnEncoder, rnnDecoder,5, lamda, threshold, loader=trainLoader, display=0)\n",
    "        score_deve,paths_deve = evaluateCorpus(rnnEncoder, rnnDecoder, 5, lamda, threshold, loader=deveLoader, display=0)\n",
    "        score_test,paths_test = evaluateCorpus(rnnEncoder, rnnDecoder, 5, lamda, threshold, loader=testLoader, display=0)\n",
    "        score_list[0].append(score_train)\n",
    "        score_list[1].append(score_deve)\n",
    "        score_list[2].append(score_test)\n",
    "        print(score_train, score_deve, score_test)\n",
    "        distinctEval(paths_deve)\n",
    "        distinctEval(paths_test)\n",
    "\n",
    "# score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 20 13:51:23 2018\n",
      "1 \t 0.2\n",
      "0.8599463678148406 0.03577600717412278 0.03621745163534304\n",
      "total length of response: 15477\n",
      "distinct unigrams: 0.11565548878981714\n",
      "distinct bigrams: 0.43367577695935905\n",
      "total length of response: 15451\n",
      "distinct unigrams: 0.1179211701507993\n",
      "distinct bigrams: 0.4388712704679309\n",
      "Sun May 20 14:10:36 2018\n",
      "1 \t 0.4\n",
      "0.8533362014076419 0.03697943346296261 0.03621004072653505\n",
      "total length of response: 15289\n",
      "distinct unigrams: 0.12433775917326183\n",
      "distinct bigrams: 0.4378965269147753\n",
      "total length of response: 15461\n",
      "distinct unigrams: 0.13226828795032664\n",
      "distinct bigrams: 0.457279606752474\n",
      "Sun May 20 14:29:41 2018\n",
      "2 \t 0.2\n",
      "0.8585377417994895 0.03939858055291495 0.042143563915589766\n",
      "total length of response: 15352\n",
      "distinct unigrams: 0.11933298593017197\n",
      "distinct bigrams: 0.44482803543512245\n",
      "total length of response: 15330\n",
      "distinct unigrams: 0.12609262883235486\n",
      "distinct bigrams: 0.45714285714285713\n",
      "Sun May 20 14:49:57 2018\n",
      "2 \t 0.4\n",
      "0.8217705027980924 0.03676383185325803 0.03763490870289678\n",
      "total length of response: 15078\n",
      "distinct unigrams: 0.1360259981429898\n",
      "distinct bigrams: 0.479108635097493\n",
      "total length of response: 15058\n",
      "distinct unigrams: 0.13567538849780847\n",
      "distinct bigrams: 0.4790808872360207\n"
     ]
    }
   ],
   "source": [
    "# hyper-prrameter tuning\n",
    "lamda_list = [0.2, 0.4]  # gamma = 0.4\n",
    "thres_list = [1, 2]\n",
    "score_list = [[],[],[]]\n",
    "\n",
    "for threshold in thres_list:\n",
    "    for lamda in lamda_list:\n",
    "        print(time.asctime( time.localtime(time.time()) ))\n",
    "        print(threshold, '\\t', lamda)\n",
    "        score_train,_ = evaluateCorpus(rnnEncoder, rnnDecoder,5, lamda, threshold, loader=trainLoader, display=0)\n",
    "        score_deve,paths_deve = evaluateCorpus(rnnEncoder, rnnDecoder, 5, lamda, threshold, loader=deveLoader, display=0)\n",
    "        score_test,paths_test = evaluateCorpus(rnnEncoder, rnnDecoder, 5, lamda, threshold, loader=testLoader, display=0)\n",
    "        score_list[0].append(score_train)\n",
    "        score_list[1].append(score_deve)\n",
    "        score_list[2].append(score_test)\n",
    "        print(score_train, score_deve, score_test)\n",
    "        distinctEval(paths_deve)\n",
    "        distinctEval(paths_test)\n",
    "\n",
    "# score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
